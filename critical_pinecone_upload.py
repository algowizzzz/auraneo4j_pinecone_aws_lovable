#!/usr/bin/env python3
"""
CRITICAL TASK 3D.1: Pinecone Infrastructure Reset & Data Upload

This is THE ONLY TASK that matters right now. Everything is blocked until this is complete.

Steps:
1. WIPE all 5,179 legacy vectors from Pinecone 
2. Extract 1,733 chunk embeddings from Neo4j with metadata
3. Upload all chunks to Pinecone with validated metadata schema
4. Build reconciliation script for ongoing data integrity
5. Test basic vector similarity search functionality

CRITICAL PATH BLOCKING - NO OTHER WORK PROCEEDS UNTIL THIS IS DONE.
"""

import os
import sys
import time
import json
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('critical_pinecone_upload.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

load_dotenv()

def check_environment():
    """Critical environment validation - MUST pass before proceeding"""
    logger.info("üîç CRITICAL ENVIRONMENT VALIDATION")
    logger.info("=" * 60)
    
    required_vars = {
        'NEO4J_URI': os.getenv('NEO4J_URI'),
        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'), 
        'PINECONE_API_KEY': os.getenv('PINECONE_API_KEY'),
        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY')
    }
    
    missing_vars = []
    for var, value in required_vars.items():
        status = "‚úÖ SET" if value else "‚ùå MISSING"
        logger.info(f"{var}: {status}")
        if not value:
            missing_vars.append(var)
    
    if missing_vars:
        logger.error(f"‚ùå CRITICAL ERROR: Missing environment variables: {missing_vars}")
        sys.exit(1)
    
    logger.info("‚úÖ All critical environment variables present")
    return True

def initialize_connections():
    """Initialize Neo4j and Pinecone connections"""
    logger.info("üîå INITIALIZING CRITICAL CONNECTIONS")
    
    try:
        # Neo4j connection
        from neo4j import GraphDatabase
        neo4j_uri = os.getenv('NEO4J_URI')
        neo4j_username = os.getenv('NEO4J_USERNAME', 'neo4j')
        neo4j_password = os.getenv('NEO4J_PASSWORD')
        
        neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))
        
        # Test Neo4j connection
        with neo4j_driver.session() as session:
            result = session.run("MATCH (c:Chunk) RETURN count(c) as chunk_count")
            chunk_count = list(result)[0]["chunk_count"]
            logger.info(f"‚úÖ Neo4j connected: {chunk_count} chunks available")
            
            if chunk_count != 1733:
                logger.warning(f"‚ö†Ô∏è  Expected 1,733 chunks, found {chunk_count}")
        
        # Pinecone connection
        import pinecone
        from pinecone import Pinecone
        
        pinecone_api_key = os.getenv('PINECONE_API_KEY')
        pc = Pinecone(api_key=pinecone_api_key)
        
        # List indexes to find the correct one
        indexes = pc.list_indexes()
        logger.info(f"üìä Available Pinecone indexes: {[idx.name for idx in indexes]}")
        
        # Use the correct index name based on inspection
        index_name = "sec-rag-index"  # This contains 5,179 legacy vectors with 384 dimensions
        
        if index_name not in [idx.name for idx in indexes]:
            logger.error(f"‚ùå CRITICAL ERROR: Index '{index_name}' not found")
            logger.info(f"Available indexes: {[idx.name for idx in indexes]}")
            return None, None, None
        
        index = pc.Index(index_name)
        
        # Check current vector count
        stats = index.describe_index_stats()
        total_vectors = stats.total_vector_count
        logger.info(f"üìä Pinecone current state: {total_vectors} vectors")
        
        return neo4j_driver, pc, index
        
    except Exception as e:
        logger.error(f"‚ùå CRITICAL CONNECTION ERROR: {e}")
        return None, None, None

def wipe_pinecone_vectors(index):
    """STEP 1: WIPE all legacy vectors from Pinecone"""
    logger.info("üóëÔ∏è  STEP 1: WIPING ALL LEGACY VECTORS FROM PINECONE")
    logger.info("=" * 60)
    
    try:
        # Get current stats
        stats = index.describe_index_stats()
        total_vectors = stats.total_vector_count
        
        if total_vectors == 0:
            logger.info("‚úÖ Pinecone already empty - no vectors to wipe")
            return True
        
        logger.info(f"üéØ Target: Wipe {total_vectors} vectors")
        
        # Delete all vectors
        logger.info("üî• Deleting all vectors...")
        index.delete(delete_all=True)
        
        # Wait for deletion to complete
        logger.info("‚è≥ Waiting for deletion to complete...")
        time.sleep(10)
        
        # Verify deletion
        stats = index.describe_index_stats()
        remaining_vectors = stats.total_vector_count
        
        if remaining_vectors == 0:
            logger.info("‚úÖ SUCCESS: All vectors wiped from Pinecone")
            return True
        else:
            logger.error(f"‚ùå DELETION FAILED: {remaining_vectors} vectors remain")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå CRITICAL ERROR wiping Pinecone: {e}")
        return False

def extract_chunk_embeddings(neo4j_driver):
    """STEP 2: Extract all chunk embeddings from Neo4j with metadata"""
    logger.info("üì§ STEP 2: EXTRACTING CHUNK EMBEDDINGS FROM NEO4J")
    logger.info("=" * 60)
    
    try:
        with neo4j_driver.session() as session:
            # Query to get all chunks with embeddings and metadata
            query = """
            MATCH (s:SourceSection)-[:HAS_CHUNK]->(c:Chunk)
            WHERE c.embedding IS NOT NULL
            RETURN 
                c.chunk_id as chunk_id,
                c.text as text,
                c.embedding as embedding,
                c.chunk_index as chunk_index,
                s.ticker as company,
                s.form_type as form_type,
                s.filing_date as filing_date,
                s.filename as filename,
                s.accession_number as accession_number
            ORDER BY s.ticker, c.chunk_index
            """
            
            logger.info("üîç Executing Neo4j query...")
            result = session.run(query)
            
            chunks_data = []
            for record in result:
                # Extract embedding (assuming it's stored as a list/array)
                embedding = record["embedding"]
                if isinstance(embedding, str):
                    # If stored as string, parse it
                    embedding = json.loads(embedding)
                
                # Clean metadata - handle null values
                metadata = {}
                if record["chunk_index"] is not None:
                    metadata["chunk_index"] = record["chunk_index"]
                if record["company"] is not None:
                    metadata["company"] = str(record["company"])
                if record["form_type"] is not None:
                    metadata["form_type"] = str(record["form_type"])
                if record["filing_date"] is not None:
                    metadata["filing_date"] = str(record["filing_date"])
                if record["filename"] is not None:
                    metadata["filename"] = str(record["filename"])
                if record["accession_number"] is not None:
                    metadata["accession_number"] = str(record["accession_number"])
                
                chunk_data = {
                    "chunk_id": record["chunk_id"],
                    "text": record["text"],
                    "embedding": embedding,
                    "metadata": metadata
                }
                chunks_data.append(chunk_data)
            
            logger.info(f"‚úÖ SUCCESS: Extracted {len(chunks_data)} chunks with embeddings")
            
            if len(chunks_data) == 0:
                logger.error("‚ùå CRITICAL ERROR: No chunks with embeddings found in Neo4j")
                return None
            
            # Validate embedding dimensions
            if chunks_data:
                sample_embedding = chunks_data[0]["embedding"]
                embedding_dim = len(sample_embedding)
                logger.info(f"üìä Embedding dimensions: {embedding_dim}")
                
                if embedding_dim != 384:
                    logger.warning(f"‚ö†Ô∏è  Expected 384 dimensions, got {embedding_dim}")
            
            return chunks_data
            
    except Exception as e:
        logger.error(f"‚ùå CRITICAL ERROR extracting chunks: {e}")
        return None

def upload_chunks_to_pinecone(index, chunks_data):
    """STEP 3: Upload all chunks to Pinecone with validated metadata"""
    logger.info("üì§ STEP 3: UPLOADING CHUNKS TO PINECONE")
    logger.info("=" * 60)
    
    try:
        total_chunks = len(chunks_data)
        batch_size = 100  # Pinecone recommended batch size
        
        logger.info(f"üéØ Target: Upload {total_chunks} chunks in batches of {batch_size}")
        
        uploaded_count = 0
        batch_count = 0
        
        for i in range(0, total_chunks, batch_size):
            batch = chunks_data[i:i + batch_size]
            batch_count += 1
            
            # Prepare vectors for Pinecone
            vectors_to_upload = []
            for chunk in batch:
                vector = {
                    "id": chunk["chunk_id"],
                    "values": chunk["embedding"],
                    "metadata": chunk["metadata"]
                }
                vectors_to_upload.append(vector)
            
            logger.info(f"üì¶ Uploading batch {batch_count}: {len(vectors_to_upload)} vectors")
            
            # Upload batch
            index.upsert(vectors=vectors_to_upload)
            uploaded_count += len(vectors_to_upload)
            
            logger.info(f"‚úÖ Batch {batch_count} uploaded: {uploaded_count}/{total_chunks} total")
            
            # Small delay between batches
            time.sleep(1)
        
        logger.info("‚è≥ Waiting for index to update...")
        time.sleep(10)
        
        # Verify upload
        stats = index.describe_index_stats()
        final_count = stats.total_vector_count
        
        if final_count == total_chunks:
            logger.info(f"‚úÖ SUCCESS: All {total_chunks} chunks uploaded to Pinecone")
            return True
        else:
            logger.error(f"‚ùå UPLOAD MISMATCH: Expected {total_chunks}, got {final_count}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå CRITICAL ERROR uploading to Pinecone: {e}")
        return False

def build_reconciliation_script(neo4j_driver, index):
    """STEP 4: Build reconciliation script for ongoing data integrity"""
    logger.info("üîç STEP 4: BUILDING RECONCILIATION SCRIPT")
    logger.info("=" * 60)
    
    reconciliation_script = '''#!/usr/bin/env python3
"""
RECONCILIATION SCRIPT: Neo4j-Pinecone Data Integrity Validation

This script validates that every Chunk node in Neo4j has a corresponding
vector in Pinecone with matching chunk_id and consistent metadata.

Run this regularly to maintain data integrity.
"""

import os
from dotenv import load_dotenv
from neo4j import GraphDatabase
from pinecone import Pinecone

load_dotenv()

def run_reconciliation():
    """Run full reconciliation between Neo4j and Pinecone"""
    print("üîç RECONCILIATION: Neo4j ‚Üî Pinecone Data Integrity Check")
    print("=" * 70)
    
    # Connect to Neo4j
    neo4j_uri = os.getenv('NEO4J_URI')
    neo4j_username = os.getenv('NEO4J_USERNAME', 'neo4j')
    neo4j_password = os.getenv('NEO4J_PASSWORD')
    neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))
    
    # Connect to Pinecone
    pinecone_api_key = os.getenv('PINECONE_API_KEY')
    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index("sec-rag-index")  # Use the correct index name
    
    # Get all chunk IDs from Neo4j
    with neo4j_driver.session() as session:
        result = session.run("MATCH (c:Chunk) RETURN c.chunk_id as chunk_id")
        neo4j_chunk_ids = set(record["chunk_id"] for record in result)
    
    print(f"üìä Neo4j chunks found: {len(neo4j_chunk_ids)}")
    
    # Get Pinecone stats
    stats = index.describe_index_stats()
    pinecone_count = stats.total_vector_count
    print(f"üìä Pinecone vectors found: {pinecone_count}")
    
    # Check for missing vectors in Pinecone
    missing_in_pinecone = []
    for chunk_id in neo4j_chunk_ids:
        try:
            result = index.fetch(ids=[chunk_id])
            if chunk_id not in result.vectors:
                missing_in_pinecone.append(chunk_id)
        except:
            missing_in_pinecone.append(chunk_id)
    
    # Report results
    print(f"\\nüìã RECONCILIATION RESULTS:")
    print(f"‚úÖ Chunks in sync: {len(neo4j_chunk_ids) - len(missing_in_pinecone)}")
    print(f"‚ùå Missing in Pinecone: {len(missing_in_pinecone)}")
    
    if missing_in_pinecone:
        print(f"\\n‚ö†Ô∏è  MISSING CHUNK IDs:")
        for chunk_id in missing_in_pinecone[:10]:  # Show first 10
            print(f"  - {chunk_id}")
        if len(missing_in_pinecone) > 10:
            print(f"  ... and {len(missing_in_pinecone) - 10} more")
    
    # Overall status
    if len(missing_in_pinecone) == 0:
        print(f"\\nüéâ SUCCESS: Perfect data integrity maintained!")
        return True
    else:
        print(f"\\n‚ö†Ô∏è  WARNING: Data integrity issues detected")
        return False

if __name__ == "__main__":
    run_reconciliation()
'''
    
    # Write reconciliation script
    script_path = "/Users/saadahmed/Desktop/Apps/AWS_Extra/SEC_Graph/reconciliation_script.py"
    with open(script_path, 'w') as f:
        f.write(reconciliation_script)
    
    logger.info(f"‚úÖ Reconciliation script created: {script_path}")
    
    # Test the reconciliation immediately
    logger.info("üß™ Testing reconciliation script...")
    try:
        # Get Neo4j chunk count
        with neo4j_driver.session() as session:
            result = session.run("MATCH (c:Chunk) RETURN count(c) as count")
            neo4j_count = list(result)[0]["count"]
        
        # Get Pinecone count
        stats = index.describe_index_stats()
        pinecone_count = stats.total_vector_count
        
        logger.info(f"üìä Neo4j chunks: {neo4j_count}")
        logger.info(f"üìä Pinecone vectors: {pinecone_count}")
        
        if neo4j_count == pinecone_count:
            logger.info("‚úÖ RECONCILIATION SUCCESS: Counts match")
            return True
        else:
            logger.error(f"‚ùå RECONCILIATION ERROR: Count mismatch")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Reconciliation test failed: {e}")
        return False

def test_vector_search(index):
    """STEP 5: Test basic vector similarity search functionality"""
    logger.info("üß™ STEP 5: TESTING VECTOR SIMILARITY SEARCH")
    logger.info("=" * 60)
    
    try:
        # Test query
        test_query = "risk management and credit exposure"
        
        # For testing, we'll create a dummy embedding
        # In production, you'd use your embedding model
        import numpy as np
        test_embedding = np.random.rand(384).tolist()  # Adjust dimension as needed
        
        logger.info(f"üîç Test query: '{test_query}'")
        
        # Perform similarity search
        results = index.query(
            vector=test_embedding,
            top_k=5,
            include_metadata=True
        )
        
        logger.info(f"üìä Search results: {len(results.matches)} matches found")
        
        for i, match in enumerate(results.matches):
            score = match.score
            chunk_id = match.id
            metadata = match.metadata
            company = metadata.get("company", "Unknown")
            
            logger.info(f"  {i+1}. {chunk_id} (Score: {score:.3f}) - {company}")
        
        if len(results.matches) > 0:
            logger.info("‚úÖ SUCCESS: Vector similarity search working")
            return True
        else:
            logger.error("‚ùå ERROR: No search results returned")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå CRITICAL ERROR testing vector search: {e}")
        return False

def main():
    """CRITICAL TASK 3D.1 - Main execution"""
    logger.info("üö® CRITICAL TASK 3D.1: PINECONE INFRASTRUCTURE RESET & DATA UPLOAD")
    logger.info("=" * 80)
    logger.info("THIS IS THE ONLY TASK THAT MATTERS - EVERYTHING IS BLOCKED UNTIL COMPLETE")
    logger.info("=" * 80)
    
    start_time = time.time()
    
    # Environment check
    if not check_environment():
        sys.exit(1)
    
    # Initialize connections
    neo4j_driver, pinecone_client, index = initialize_connections()
    if not all([neo4j_driver, pinecone_client, index]):
        logger.error("‚ùå CRITICAL ERROR: Could not establish connections")
        sys.exit(1)
    
    try:
        # STEP 1: Wipe Pinecone
        if not wipe_pinecone_vectors(index):
            logger.error("‚ùå CRITICAL FAILURE: Could not wipe Pinecone vectors")
            sys.exit(1)
        
        # STEP 2: Extract chunks from Neo4j  
        chunks_data = extract_chunk_embeddings(neo4j_driver)
        if not chunks_data:
            logger.error("‚ùå CRITICAL FAILURE: Could not extract chunks from Neo4j")
            sys.exit(1)
        
        # STEP 3: Upload to Pinecone
        if not upload_chunks_to_pinecone(index, chunks_data):
            logger.error("‚ùå CRITICAL FAILURE: Could not upload chunks to Pinecone")
            sys.exit(1)
        
        # STEP 4: Build reconciliation script
        if not build_reconciliation_script(neo4j_driver, index):
            logger.error("‚ùå WARNING: Could not build reconciliation script")
        
        # STEP 5: Test vector search
        if not test_vector_search(index):
            logger.error("‚ùå WARNING: Vector search test failed")
        
        # SUCCESS
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info("üéâ CRITICAL TASK 3D.1 COMPLETE")
        logger.info("=" * 60)
        logger.info(f"‚úÖ All legacy vectors wiped from Pinecone")
        logger.info(f"‚úÖ All {len(chunks_data)} chunks uploaded to Pinecone")
        logger.info(f"‚úÖ Reconciliation script created and tested")
        logger.info(f"‚úÖ Vector similarity search validated")
        logger.info(f"‚è±Ô∏è  Total execution time: {duration:.2f} seconds")
        logger.info("")
        logger.info("üöÄ UNBLOCKED: All other tasks can now proceed")
        logger.info("üöÄ RAG and Hybrid nodes are now functional")
        logger.info("üöÄ Ready for comprehensive E2E testing")
        
    except KeyboardInterrupt:
        logger.info("‚ö†Ô∏è  Task interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå CRITICAL UNEXPECTED ERROR: {e}")
        sys.exit(1)
    finally:
        if neo4j_driver:
            neo4j_driver.close()

if __name__ == "__main__":
    main()