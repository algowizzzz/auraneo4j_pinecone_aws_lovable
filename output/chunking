### Suggested Chunking Strategy for Graph RAG (Neo4j) and Embeddings (Pinecone)

Based on the structure of your output folder (which contains 34+ markdown files of 10-K filings, with hierarchical headers like #, ##, ###, ####, and content including text, tables, and financial data), I'll outline a comprehensive, efficient chunking approach tailored for **Graph RAG in Neo4j** (for structured entity/relationship queries) and **vector embeddings in Pinecone** (for semantic similarity search). This enables hybrid retrieval: combining graph-based navigation (e.g., "Find all risk factors related to AXP") with vector search (e.g., "Similar sections on operational risks").

The goal is to create **meaningful, retrievable chunks** that preserve context, handle varying section lengths (from our prior analysis: median ~245 words, but some >20k), and optimize for RAG efficiency (e.g., avoid chunks too small for context or too large for embedding models like those with 512-token limits).

#### **1. Why This Chunking Approach?**
- **Challenges from Your Data**:
  - Files are long (e.g., 2.48M total words across 4,892 sections).
  - Hierarchical structure (headers) indicates logical breaks—ideal for chunking.
  - Content includes dense financial text, tables, and repetitive sections (e.g., "Critical Audit Matters" >20k words).
  - For Graph RAG: Need to extract entities (e.g., companies like AXP, metrics like "revenue") and relationships (e.g., "Section X belongs to Filing Y").
  - For Pinecone: Embeddings work best on coherent ~200-1000 word chunks for semantic retrieval.
- **Benefits**:
  - **Efficient Retrieval**: Hybrid setup allows fast vector search (Pinecone) + structured queries (Neo4j).
  - **Scalable**: Handles your 34 files; easy to extend.
  - **RAG-Optimized**: Chunks with metadata enable precise, context-aware responses.

#### **2. Recommended Chunking Strategy**
Use a **hybrid header-based + size-based chunking** approach. This respects the document's natural structure while ensuring chunks are embedding-friendly. Implement in Python using libraries like LangChain (for chunking/RAG) or custom scripts (building on our prior `parse_10k_sections.py`).

##### **Step-by-Step Chunking Process:**
1. **Preprocess Files**:
   - Scan all `.md` files in `output/` (use `os.walk` as in our prior scripts).
   - Clean content: Remove excessive whitespace, convert tables to text (e.g., using `markdown` library or regex to flatten), strip artifacts (e.g., page numbers, footers).
   - Extract metadata per file: Company ticker (from path, e.g., "AXP"), year (e.g., "2024"), filing type ("10-K").

2. **Primary Chunking Method: Hierarchical Header Splitting**:
   - **Split by Headers**: Break files into chunks based on markdown headers.
     - **Level 1 (#)**: Major parts (e.g., "PART I").
     - **Level 2 (##)**: Items (e.g., "ITEM 1. BUSINESS").
     - **Level 3 (###)**: Subsections (e.g., "Overview").
     - **Level 4 (####)**: Detailed subsections (e.g., "Our Integrated Payments Platform").
   - **Why?** From our analysis, level 4 sections are granular (avg ~508 words) but vary widely—perfect for chunks. Combine levels if a section is too small (<100 words) or split if too large (>2000 words).
   - **Fallback Splitting** (for long sections):
     - If a section >2000 words, split into sub-chunks by paragraphs or sentences (e.g., every 500-800 words).
     - Use overlapping (e.g., 20% overlap) to maintain context.
   - **Chunk Size Target**: 200-1500 words (optimal for embeddings; adjust based on model token limits).

3. **Handle Special Content**:
   - **Tables**: Convert to structured text (e.g., "Key: Value | Key: Value") or embed as separate chunks with metadata like "table: financial_metrics".
   - **Lists/Bullets**: Keep intact within chunks to preserve meaning.
   - **Empty/Short Sections**: Merge with parent (e.g., sections with 0 words from prior analysis).

4. **Add Metadata to Each Chunk**:
   - **Essential Fields**:
     - `id`: Unique (e.g., "AXP_2024_ITEM1_Overview_chunk1").
     - `company`: Ticker (e.g., "AXP").
     - `year`: Filing year (e.g., "2024").
     - `hierarchy`: Header path (e.g., "PART I > ITEM 1 > Overview").
     - `word_count`: From our prior script.
     - `source_file`: Full path.
   - **Why?** Enables filtered retrieval (e.g., "Query only 2024 filings for BAC").

5. **Output Chunks**:
   - Save as JSON/CSV per file or aggregated (e.g., `chunks/all_chunks.json`).
   - Total expected chunks: ~5,000-6,000 (based on 4,892 sections, plus splits for long ones).

##### **Code Snippet: Basic Chunking Script**
Build on our existing `parse_10k_sections.py`. Here's a starter (expand as needed):

```python
import os
import re
from pathlib import Path

def chunk_by_headers(file_path, max_chunk_words=1500):
    with open(file_path, 'r') as f:
        content = f.read()
    
    # Split by level 4 headers (as before)
    sections = []  # Use parse_10k_by_h4_headers from prior script
    
    chunks = []
    for section in sections:
        text = clean_text(section['content'])  # From prior script
        words = text.split()
        
        # Split if too long
        if len(words) > max_chunk_words:
            for i in range(0, len(words), max_chunk_words - 100):  # Overlap by 100 words
                chunk_text = ' '.join(words[i:i + max_chunk_words])
                chunks.append({
                    'text': chunk_text,
                    'metadata': {
                        'section': section['name'],
                        'company': Path(file_path).parent.parent.name,
                        'word_count': len(chunk_text.split())
                    }
                })
        else:
            chunks.append({
                'text': text,
                'metadata': {...}  # Add metadata
            })
    
    return chunks

# Process all files
for file in find_all_10k_files('output'):
    chunks = chunk_by_headers(file)
    # Save or process
```

#### **3. Integration with Neo4j (Graph RAG)**
- **Graph Schema**:
  - **Nodes**: Company (e.g., AXP), Filing (e.g., 2024_10K), Section (e.g., "Risk Factors"), Entity (e.g., "Revenue" extracted via LLM).
  - **Relationships**: COMPANY_HAS_FILING, FILING_HAS_SECTION, SECTION_MENTIONS_ENTITY, ENTITY_RELATES_TO (e.g., "Revenue impacts Expenses").
- **Building the Graph**:
  1. For each chunk, use an LLM (e.g., OpenAI GPT-4) to extract entities/relations (prompt: "Extract key financial entities and relationships from this text").
  2. Load into Neo4j using Cypher queries (e.g., `CREATE (c:Company {name: 'AXP'})-[:HAS_FILING]->(f:Filing {year: 2024})`).
  3. Index nodes for fast queries.
- **RAG Benefits**: Enables traversals like "Find all companies with high-risk sections related to cybersecurity".

#### **4. Integration with Pinecone (Embeddings)**
- **Setup**:
  - Create Pinecone index (dimensionality: 384 for MiniLM, or 1536 for OpenAI).
  - Use embedding model: `sentence-transformers/all-MiniLM-L6-v2` (fast, open-source) or OpenAI `text-embedding-ada-002`.
- **Embedding & Upsert**:
  1. Embed each chunk's text.
  2. Upsert to Pinecone: Vector + metadata (e.g., `pinecone.upsert([(chunk_id, embedding, metadata)])`).
- **Retrieval**: Query with hybrid search (vector similarity + metadata filters, e.g., "company=AXP AND year=2024").

#### **5. Full Pipeline & Tools**
- **Libraries**: LangChain (for chunking/LLM extraction), Neo4j Python driver, Pinecone SDK, HuggingFace Transformers (for embeddings).
- **Pipeline Script**:
  1. Chunk all files.
  2. Extract entities/relations (LLM batch processing).
  3. Load to Neo4j.
  4. Embed and upsert to Pinecone.
- **Efficiency Tips**:
  - **Batch Processing**: Process files in parallel (use `concurrent.futures`).
  - **Cost Optimization**: Use local models for embeddings to avoid API costs.
  - **Validation**: Test retrieval on sample queries (e.g., accuracy >80%).
  - **Scalability**: Your dataset (~2.5M words) fits in free tiers; monitor for larger corpora.

#### **6. Potential Improvements & Risks**
- **Improvements**: Add semantic chunking (e.g., via LLM to detect topic shifts) for better coherence.
- **Risks**: Over-chunking may lose context; under-chunking may hit token limits. Test with a subset first.
- **Next Steps**: If you provide API keys (Neo4j URI, Pinecone key), I can generate a full script to run this.

This strategy balances structure, efficiency, and retrieval quality—let me know if you'd like code or refinements!