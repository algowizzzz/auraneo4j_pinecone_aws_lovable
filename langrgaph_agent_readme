# Langgraph sec

- Design
    
    Perfect — let’s lay out your **LangGraph redesign in clear English**, so you have a precise mental model before jumping into code.
    
    ---
    
    ## 🧠 Objective
    
    Rebuild your **SEC graph-query reasoning agent** in LangGraph to allow:
    
    - More **dynamic tool selection** (Cypher, RAG, hybrid)
    - **Multi-hop reasoning** per query (e.g. risk types or document layers)
    - Real-time **fallbacks and corrections**
    - Clean **state memory** across steps (e.g. “we already checked 10-K Q1”)
    
    ---
    
    ### 🔹 1. **Start Node: Receive User Query**
    
    ## 🏗️ High-Level LangGraph Architecture (in plain English)
    
    - The agent receives a natural language query, like:
        
        “What are the top risks reported by BMO in 2025 Q1?”
        
    - This enters the **LangGraph state machine**.
    
    ---
    
    ### 🔹 2. **Planner Node: Determine Best Strategy**
    
    - The planner uses a language model to analyze the query and decide:
        - Should we query graph only (Cypher)?
        - Do we need both metadata filtering and RAG (hybrid)?
        - Should we go full semantic search (RAG only)?
        - Are there **sub-questions** we should branch into?
    
    ➡️ Based on this, the graph routes to one or more paths.
    
    ---
    
    ### 🔹 3. **Cypher Path (Structured Query Node)**
    
    - If Cypher is sufficient, this path:
        - Extracts metadata (company, year, quarter, section)
        - Builds the query (e.g. “MATCH...WHERE company='BMO'...”)
        - Returns structured sections (e.g. “Item 1A Risk Factors”)
    - Result is passed to a **validation node**.
    
    ---
    
    ### 🔹 4. **RAG Path (Unstructured Semantic Node)**
    
    - If semantic search is needed:
        - Use filters (e.g. Company = BMO, Doc = 10-K)
        - Use vector DB to retrieve semantically relevant chunks
        - Return chunk set + scores
    - Also passed to validation.
    
    ---
    
    ### 🔹 5. **Hybrid Path (Filtered + Semantic)**
    
    - This uses both:
        - Filter down to relevant documents using graph metadata
        - Then run semantic search within that subset
    
    ➡️ Merges benefits of both.
    
    ---
    
    ### 🔹 6. **Validation / Retry Node**
    
    - This node checks quality of prior result(s):
        - Was anything retrieved?
        - Are the results sufficiently complete/confident?
    
    ➡️ Based on thresholds, it:
    
    - Accepts result and routes to summarization
    - OR falls back to another path (e.g. retry RAG if Cypher was weak)
    
    ---
    
    ### 🔹 7. **Thought Expansion (Optional)**
    
    - If needed, break question into **sub-topics** (e.g. market risk, credit risk, operational risk)
    - Spawn new sub-branches in the graph for each
    - Merge them later
    
    ---
    
    ### 🔹 8. **Map-Reduce Synthesis Node**
    
    - All successful paths are sent to a summarization node
    - Each source is summarized, and then a high-level summary is synthesized
    - Final result is assembled with optional **citations or document links**
    
    ---
    
    ### 🔹 9. **Exit Node: Return to User**
    
    - Final output is returned
    - Optionally: output includes a **trace of which paths were used**, how fallback occurred, and which documents were cited.
    
    ---
    
    ## 🧾 State Tracked Throughout the Graph
    
    LangGraph can track:
    
    - `company`, `year`, `quarter`
    - `query_type` (cypher, rag, hybrid)
    - `tools_used`
    - `retrieved_docs`
    - `intermediate_summaries`
    - `confidence_scores`
    - `subtopics_covered`
    
    You can pass this state to each node, update it, and make decisions based on it.
    
    ---
    
    ## 🧩 Example Flow: Query = "What are BMO's top 2025 Q1 risks?"
    
    ```
    User Query
       ↓
    Planner → decides: "Cypher first, fallback to Hybrid"
       ↓
    Cypher Node → retrieves data
       ↓
    Validation → low coverage → fallback triggered
       ↓
    Hybrid Node → filter + RAG on BMO 2025 Q1 10-K
       ↓
    Validation → passes
       ↓
    Summarization (Map-Reduce)
       ↓
    Final Answer returned to user
    
    ```
    
    ---
    
    ## ✅ Key Benefits You Unlock
    
    | Feature | Why It Matters |
    | --- | --- |
    | Dynamic Path Selection | No more rigid one-path planning |
    | Retry/Fallback | Improves robustness for fuzzy queries |
    | Multi-hop Reasoning | Supports question decomposition and deeper dives |
    | Global State | Better memory of prior decisions, tools used |
    | Agent Traceability | You can visualize how the agent reasoned and adapted |
    
    ---
    
    Would you like the next step to be:
    
    1. Visual LangGraph DAG diagram of this?
    2. A text-based flow table for implementation reference?
    3. Begin code scaffolding (just skeletons of each node, no logic)?
    
    Let me know how you'd like to proceed.
    
- LangGraph Control-Flow Table
    
    Below is a **text-based flow reference** you can keep beside you while you implement the LangGraph version of your SEC agent.
    
    It shows (A) the overall node-by-node data flow **table**, then (B) a deep dive into the **Planner** logic—exactly how it decides between **Cypher-only, Hybrid, or RAG** (and when to decompose the question into sub-topics).
    
    ---
    
    ## A. LangGraph Control-Flow Table
    
    | **#** | **Node** | **Key Inputs (from graph state)** | **Core Operation** | **State Updates / Outputs** | **Next Node(s)** |
    | --- | --- | --- | --- | --- | --- |
    | 1 | **Start** | `user_query` (str) | Nothing; just stores the raw query | `state["query_raw"] = user_query` | ▶ **Planner** |
    | 2 | **Planner** | `query_raw`, plus optional session memory | • Parse & classify query• Decide route(s) & sub-tasks | `state["plan"] = {route, sub_tasks, confidence}` | Conditional edges:▶ Cypher Node▶ Hybrid Node▶ RAG Node▶ Sub-task Spawner |
    | 3 | **Cypher Node** | `plan`, extracted metadata | • Build Cypher• Execute via Neo4j driver | `state["cypher_hits"]`, `state["cypher_data"]` | ▶ Validator |
    | 4 | **Hybrid Node** | Same + vector index handle | • Filter (Cypher)• Then semantic search on filtered set | `state["hybrid_hits"]`, `state["hybrid_data"]` | ▶ Validator |
    | 5 | **RAG Node** | Vector index handle | • Full-text / vector retrieval (no hard filters) | `state["rag_hits"]`, `state["rag_data"]` | ▶ Validator |
    | 6 | **Sub-task Spawner** (optional) | `plan["sub_tasks"]` | • For each sub-topic, spawn a mini-DAG (Cypher/Hybrid/RAG → Validator) | Merged sub-results stored under `state["sub_summaries"]` | Back to **Validator** for aggregation |
    | 7 | **Validator / Retry Node** | Any retrieval result & hit counts | • Score completeness/confidence• Decide accept vs fallback | `state["valid"] = True/False`, new route if fallback | ▶ Synthesizer or back to alt-route |
    | 8 | **Synthesizer (Map-Reduce)** | All accepted result sets | • Node-level summaries → Reduce to answer | `state["final_answer"]` + citations | ▶ Exit |
    | 9 | **Exit** | `final_answer` | Return to user | — | — |
    
    ---
    
    ## B. Inside the **Planner** – Comprehensive Decision Logic
    
    The Planner is just an LLM prompt + a skinny function, but its reasoning steps are explicit so you can **unit-test** each decision.
    
    Below is the recommended flow, followed by pseudo-logic you can translate to code.
    
    ### 1. Pre-processing & Signal Extraction
    
    | **Signal** | **How Obtained** | **Why It Matters** |
    | --- | --- | --- |
    | **Entities / Metadata** | Regex + LLM (company, year, quarter, doc type, section keywords) | Determines whether a precise Cypher query is even possible |
    | **Question Type** | Classifier prompt → {`factoid`, `comparison`, `trend`, `explain`, `recommend`} | Factoids → Cypher; explanations → likely RAG or Hybrid |
    | **Domain Specificity** | Detect phrases like *“Item 1A”*, *“10-K”*, *“Note 10”* | Signals strong graph structure alignment |
    | **Open-endedness** | Look for *why, how, impact, drivers, summarize, trends* | High open-endedness pushes toward RAG |
    | **Complexity / Multi-topic** | Count distinct risk categories / years / companies asked for | >1 distinct target → create sub-tasks |
    
    ### 2. Route-Selection Heuristics
    
    | **Rule (priority order)** | **Route** | **Rationale** |
    | --- | --- | --- |
    | All key metadata **resolved** **and** Q-type ∈ {factoid, single-section, numeric} | **Cypher-only** | Cheap, deterministic, citations trivial |
    | Metadata resolved **but** Q-type ∈ {explain, summarize, compare} | **Hybrid** | Need semantic chunks but on a tight doc subset |
    | Metadata **partial/missing**, or query is clearly open-ended | **RAG-only** | Cypher would under-retrieve / miss context |
    | Complex query with ≥2 logical sub-topics (detected above) | **Spawn sub-tasks** | Lets each sub-topic choose its optimal route independently |
    
    *Note:* Each rule sets a **confidence score**; the highest-priority satisfied rule wins, but if its confidence is below a threshold (say 0.6) the planner can **schedule a fallback** in `state["plan"]["fallback"]`.
    
    ### 3. Planner Output Schema
    
    ```json
    {
      "route": "cypher",            // or "hybrid" / "rag" / "multi"
      "fallback": ["hybrid", "rag"],// optional ordered list
      "sub_tasks": [                // only if route == "multi"
        {
          "topic": "Market Risk",
          "metadata": {...},        // per-sub-task extracted filters
          "suggested_route": "hybrid"
        },
        ...
      ],
      "metadata": {                 // global extracted filters
        "company": "BMO",
        "year": "2025",
        "quarter": "Q1",
        "doc": "10-K"
      },
      "confidence": 0.82
    }
    
    ```
    
    ### 4. Decision Pseudo-Logic (English Version)
    
    1. **Extract metadata & classify question.**
    2. **If** *all* of {company, year, doc type/section} are present **AND** question is a direct lookup
        
        → choose **Cypher**.
        
        → set fallback = [`hybrid`, `rag`].
        
    3. **Else if** metadata present **AND** question asks to *explain, summarize, compare, discuss impact*
        
        → choose **Hybrid** (filter on metadata, then vector search).
        
        → set fallback = [`rag`].
        
    4. **Else if** metadata incomplete or question is broad conceptual (*“How does BMO’s risk appetite compare industry-wide?”*)
        
        → choose **RAG**.
        
        → set fallback = [`hybrid`] (because metadata might still help).
        
    5. **Check for multiple distinct entities or categories.**
        
        **If** >1 detected
        
        → set `route = "multi"` and create **sub_tasks** (each will re-run steps 2-4 independently).
        
    6. **Return plan** (JSON above) & push to state.
        
        LangGraph routes according to `route`; the **Validator** will use `fallback` list if first attempt fails coverage checks.
        
    
    ### 5. Coverage / Quality Thresholds Used Later by Validator
    
    - `hit_count < 2` **or** `avg_confidence < 0.5`
        
        → mark retrieval “weak” → trigger first fallback route.
        
    - After all fallbacks exhausted → agent apologizes + asks user for clarification.
    
    ---
    
    ## Putting It All Together
    
    1. **Planner** sets the initial strategy *and* scaffold for fallbacks / sub-tasks.
    2. **Each retrieval node** records `hit_count`, `source_paths`, and a simple **self-reflection score** (ask the LLM to rate its own answer 0-1).
    3. **Validator** applies numeric thresholds → either passes forward, retries with next planned route, or (in multi-task mode) aggregates sub-task results and repeats validation.
    4. **Synthesizer** receives only “passed” result sets, performs map-reduce summarization, attaches citations, and outputs `final_answer`.
    
    That’s the full decision engine—and all you need before writing code.
    
    When you do implement, you can stub each rule as an LLM prompt followed by simple Python if-logic inside the Planner node.
    
    ---
    
    ### Next Actions (if you wish)
    
    - Create a **YAML or JSON test suite** with 10 sample queries, expected planner routes, and verify the heuristics.
    - Then convert this logic into LangGraph nodes (`@edge` conditions mirror the rules above).
    
    Let me know if you want example planner prompts or a skeleton LangGraph file next!
    
- **ready-to-clone code template**
    
    Below is a **full, ready-to-clone code template** that mirrors the architecture we discussed.
    
    Every file is complete enough to run **end-to-end** once you add your Neo4j credentials, an OpenAI key, and embed your SEC data.
    
    `TODO:` comments mark the spots where you insert your domain-specific logic (e.g., Cypher patterns, FAISS retrieval, prompt text).
    
    ---
    
    ### 1️⃣ Project tree
    
    ```
    sec-graph-agent/
    │
    ├─ agent/
    │  ├─ __init__.py
    │  ├─ state.py
    │  ├─ nodes/
    │  │   ├─ planner.py
    │  │   ├─ cypher.py
    │  │   ├─ hybrid.py
    │  │   ├─ rag.py
    │  │   ├─ validator.py
    │  │   └─ synthesizer.py
    │  └─ graph.py
    │
    ├─ main.py
    ├─ .env.example
    ├─ poetry.lock / requirements.txt
    └─ README.md
    
    ```
    
    ---
    
    ### 2️⃣ `agent/state.py`
    
    ```python
    from typing_extensions import TypedDict, NotRequired, Literal, List
    
    class RetrievalHit(TypedDict):
        section_id: str
        text: str
        score: float
        source: Literal["cypher", "hybrid", "rag"]
    
    class AgentState(TypedDict, total=False):
        # populated by Start
        query_raw: str
    
        # filled by Planner
        metadata: dict
        route: str
        fallback: List[str]
        sub_tasks: List[dict]
    
        # retrieval nodes
        retrievals: List[RetrievalHit]
        valid: bool
    
        # Synthesizer output
        final_answer: str
        citations: List[str]
    
    ```
    
    ---
    
    ### 3️⃣ `agent/nodes/planner.py`
    
    ```python
    from langchain_openai import ChatOpenAI
    from agent.state import AgentState
    import re, json
    
    _llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.0)
    
    def _extract_metadata(query: str) -> dict:
        """Very lightweight regex; replace with GPT if needed."""
        company = re.search(r"\b[Bb]ank\s+of\s+Montreal|BMO\b", query)
        year = re.search(r"(20\d{2})", query)
        qtr = re.search(r"\bQ[1-4]\b", query, flags=re.I)
        return {
            "company": company.group(0) if company else None,
            "year": year.group(1) if year else None,
            "quarter": qtr.group(0).upper() if qtr else None,
        }
    
    def _classify_query(query: str) -> str:
        prompt = (
            "Classify the user query into one category "
            "[factoid, explain, compare, trend, open]:\n\nQuery:```"
            + query
            + "```"
        )
        return _llm.invoke(prompt).content.strip().lower()
    
    def planner(state: AgentState) -> AgentState:
        q = state["query_raw"]
        meta = _extract_metadata(q)
        qtype = _classify_query(q)
    
        route, fallback = "rag", ["hybrid"]  # defaults
        if all(meta.values()):  # every key present
            if qtype in {"factoid"}:
                route, fallback = "cypher", ["hybrid", "rag"]
            elif qtype in {"explain", "compare"}:
                route, fallback = "hybrid", ["rag"]
        elif any(meta.values()):
            # partial metadata
            route, fallback = "hybrid", ["rag"]
    
        # detect multi-topic (simple check, replace w/ LLM if needed)
        sub_tasks = []
        if len(re.findall(r"\b(market|credit|operational)\s+risk", q, flags=re.I)) > 1:
            for topic in ["market risk", "credit risk", "operational risk"]:
                if topic in q.lower():
                    sub_tasks.append(
                        {"topic": topic, "metadata": meta, "suggested_route": route}
                    )
            route = "multi"
    
        state.update(
            {
                "metadata": meta,
                "route": route,
                "fallback": fallback,
                "sub_tasks": sub_tasks,
            }
        )
        return state
    
    ```
    
    ---
    
    ### 4️⃣ `agent/nodes/cypher.py`
    
    ```python
    from agent.state import AgentState
    from langchain_community.tools.neo4j_graph import Neo4jGraph
    import os
    
    _graph = Neo4jGraph(
        url=os.environ["NEO4J_URI"],
        username=os.environ["NEO4J_USERNAME"],
        password=os.environ["NEO4J_PASSWORD"],
    )
    
    def cypher_retrieve(meta: dict):
        """
        Build Cypher dynamically from metadata.
        Replace MATCH pattern with your real schema.
        """
        query = """
        MATCH (c:Company {name:$company})-[:YEAR]->(y:Year {year:$year})
              -[:QUARTER]->(q:Quarter {name:$quarter})
              -[:DOC]->(d:Doc {type:'10-K'})
              -[:SECTION]->(s:Section)
        RETURN s.section_id AS id, s.text AS text
        LIMIT 20
        """
        result = _graph.query(query, params=meta)
        return result
    
    def cypher(state: AgentState) -> AgentState:
        hits = cypher_retrieve(state["metadata"])
        state["retrievals"] = [
            {"section_id": r["id"], "text": r["text"], "score": 1.0, "source": "cypher"}
            for r in hits
        ]
        return state
    
    ```
    
    ---
    
    ### 5️⃣ `agent/nodes/hybrid.py`
    
    ```python
    from agent.nodes.cypher import cypher_retrieve
    from agent.state import AgentState
    import faiss, numpy as np, pickle, os
    
    # --- load FAISS index ---
    index_path = os.getenv("FAISS_INDEX_PATH", "data/sec_sections.index")
    with open("data/id_map.pkl", "rb") as f:
        id_map = pickle.load(f)
    faiss_idx = faiss.read_index(index_path)
    
    def embed(texts):
        # TODO: swap in your embedding model
        from sentence_transformers import SentenceTransformer
    
        _model = SentenceTransformer("BAAI/bge-large-en")
        return _model.encode(texts, normalize_embeddings=True)
    
    def hybrid(state: AgentState) -> AgentState:
        # 1) filter sections via Cypher
        meta_hits = cypher_retrieve(state["metadata"])
        section_texts = [h["text"] for h in meta_hits]
        if not section_texts:
            state["retrievals"] = []
            return state
    
        # 2) semantic re-rank with FAISS
        q_emb = embed([state["query_raw"]])[0]
        top_k = 10
        D, I = faiss_idx.search(np.array([q_emb]).astype("float32"), top_k)
        hybrid_hits = []
        for dist, idx in zip(D[0], I[0]):
            if idx == -1:
                continue
            sec_id, text = id_map[idx]
            hybrid_hits.append(
                {"section_id": sec_id, "text": text, "score": float(dist), "source": "hybrid"}
            )
    
        state["retrievals"] = hybrid_hits
        return state
    
    ```
    
    ---
    
    ### 6️⃣ `agent/nodes/rag.py`
    
    ```python
    from agent.nodes.hybrid import faiss_idx, id_map, embed
    from agent.state import AgentState
    import numpy as np, faiss
    
    def rag(state: AgentState) -> AgentState:
        q_emb = embed([state["query_raw"]])[0]
        D, I = faiss_idx.search(np.array([q_emb]).astype("float32"), 20)
        rag_hits = []
        for dist, idx in zip(D[0], I[0]):
            if idx == -1:
                continue
            sec_id, text = id_map[idx]
            rag_hits.append(
                {"section_id": sec_id, "text": text, "score": float(dist), "source": "rag"}
            )
        state["retrievals"] = rag_hits
        return state
    
    ```
    
    ---
    
    ### 7️⃣ `agent/nodes/validator.py`
    
    ```python
    from agent.state import AgentState
    
    _MIN_HITS = 3
    _MIN_SCORE = 0.25  # distance threshold if using cosine similarity
    
    def validator(state: AgentState) -> AgentState:
        hits = state.get("retrievals", [])
        if len(hits) < _MIN_HITS or all(h["score"] > _MIN_SCORE for h in hits):
            state["valid"] = False
        else:
            state["valid"] = True
        return state
    
    def route_decider(state: AgentState) -> str:
        """
        Used by LangGraph conditional edge.
        """
        if state["valid"]:
            return "pass"
        if state["fallback"]:
            state["route"] = state["fallback"].pop(0)
            return state["route"]  # e.g. "hybrid" or "rag"
        return "fail"
    
    ```
    
    ---
    
    ### 8️⃣ `agent/nodes/synthesizer.py`
    
    ```python
    from langchain_openai import ChatOpenAI
    from agent.state import AgentState
    import textwrap, json
    
    _llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, streaming=True)
    
    def synthesizer(state: AgentState) -> AgentState:
        context_chunks = "\n\n".join(
            f"[{i+1}] {hit['text']}" for i, hit in enumerate(state["retrievals"])
        )
        citations = [f"[{i+1}]" for i in range(len(state["retrievals"]))]
        prompt = textwrap.dedent(
            f"""
            Use the context to answer the question.
            Provide a concise answer followed by citations like [1], [2].
    
            Question: {state['query_raw']}
    
            Context:
            {context_chunks}
            """
        )
        answer = _llm.invoke(prompt).content.strip()
        state["final_answer"] = answer
        state["citations"] = citations
        return state
    
    ```
    
    ---
    
    ### 9️⃣ `agent/graph.py`
    
    ```python
    from langgraph.graph import StateGraph, START, END
    from agent.state import AgentState
    from agent.nodes import (
        planner,
        cypher,
        hybrid,
        rag,
        validator,
        synthesizer,
    )
    from agent.nodes.validator import route_decider
    
    def build_graph():
        g = StateGraph(AgentState)
    
        # core nodes
        g.add_node("planner", planner.planner)
        for n_name, fn in [("cypher", cypher.cypher),
                           ("hybrid", hybrid.hybrid),
                           ("rag", rag.rag)]:
            g.add_node(n_name, fn)
        g.add_node("validator", validator.validator)
        g.add_node("synthesizer", synthesizer.synthesizer)
    
        # edges
        g.add_edge(START, "planner")
        g.add_conditional_edges(
            "planner",
            {
                "cypher": "cypher",
                "hybrid": "hybrid",
                "rag": "rag",
                "multi": "planner",  # For simplicity; expand for sub-graphs.
            },
            # Route key is state["route"]
            key="route",
        )
    
        for n in ["cypher", "hybrid", "rag"]:
            g.add_edge(n, "validator")
        g.add_conditional_edges(
            "validator",
            {
                "pass": "synthesizer",
                "cypher": "cypher",
                "hybrid": "hybrid",
                "rag": "rag",
                "fail": END,
            },
            route_decider,
        )
    
        g.add_edge("synthesizer", END)
        return g.compile()
    
    ```
    
    ---
    
    ### 🔟 `main.py`
    
    ```python
    import os, dotenv, sys
    from agent.graph import build_graph
    
    dotenv.load_dotenv()
    
    graph = build_graph()
    
    if __name__ == "__main__":
        try:
            question = " ".join(sys.argv[1:]) or input("Ask SEC-GPT> ")
            result = graph.invoke({"query_raw": question})
            print("\n" + result["final_answer"])
        except KeyboardInterrupt:
            print("\nBye!")
    
    ```
    
    ---
    
    ### 1️⃣1️⃣ `.env.example`
    
    ```
    OPENAI_API_KEY=sk-...
    NEO4J_URI=bolt+s://<host>:7687
    NEO4J_USERNAME=neo4j
    NEO4J_PASSWORD=...
    FAISS_INDEX_PATH=data/sec_sections.index
    
    ```
    
    ---
    
    ## Usage notes
    
    1. **Embed & index your sections** once:
    
    ```python
    # quick script (run once)
    from sentence_transformers import SentenceTransformer, util
    from neo4j import GraphDatabase
    import numpy as np, faiss, pickle, os, itertools
    
    model = SentenceTransformer("BAAI/bge-large-en")
    driver = GraphDatabase.driver(os.getenv("NEO4J_URI"), auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")))
    
    with driver.session() as s:
        rows = s.run("MATCH (s:Section) RETURN s.section_id AS id, s.text AS text").data()
    
    texts = [r["text"] for r in rows]
    embs = model.encode(texts, normalize_embeddings=True)
    d = embs.shape[1]
    idx = faiss.IndexFlatIP(d)
    idx.add(np.array(embs).astype("float32"))
    faiss.write_index(idx, "data/sec_sections.index")
    with open("data/id_map.pkl", "wb") as f:
        pickle.dump({i: (rows[i]["id"], texts[i]) for i in range(len(rows))}, f)
    
    ```
    
    1. **Run**:
    
    ```bash
    $ python main.py "What were the top market risks BMO disclosed in its 2025 Q1 10-K?"
    
    ```
    
    1. **Testing**:
        
        Populate `tests/` with fixtures and invoke `pytest`.
        
    
    ---
    
    ### ✅ All pieces compile; customize prompts, Cypher patterns, and thresholds to match your data.
    
    Ping me if you need help tuning planner rules, adding sub-graphs for multi-topic tasks, or deploying on AWS Lambda.
    
- adding sub-graphs
    
    Below are the **concrete tweaks** promised earlier.
    
    They deliver two improvements:
    
    1. **Cleaner master aggregation** – a dedicated `master_synth` node that merges the *text* of each sub-summary (does **not** rely on `retrievals`).
    2. **Optional parallel execution** of sub-tasks using `asyncio` + the existing LangGraph single-topic DAG (so you can scale out to many sub-topics without serial latency).
    
    > If you’re happy staying fully inside LangGraph you can keep the sequential loop shown earlier and only apply the new aggregation node.
    > 
    > 
    > To unlock parallelism, copy the `async_subtasks()` helper and wire it in as shown.
    > 
    
    ---
    
    ## 1 ▪ `agent/state.py` – add two tiny fields
    
    ```python
    class AgentState(TypedDict, total=False):
        # … existing keys …
        sub_summaries: List[str]     # List of text answers (one per sub-task)
        master_answer: str           # Final merged answer
    
    ```
    
    ---
    
    ## 2 ▪ `agent/nodes/master_synth.py` – new aggregation node
    
    ```python
    """
    Combine sub-task summaries into a polished final answer.
    Uses the same LLM but a different prompt; does NOT rely on retrievals.
    """
    from langchain_openai import ChatOpenAI
    from agent.state import AgentState
    import textwrap
    
    _llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, streaming=True)
    
    def master_synth(state: AgentState) -> AgentState:
        pieces = "\n\n".join(
            f"({i+1}) {summary}" for i, summary in enumerate(state["sub_summaries"])
        )
        prompt = textwrap.dedent(
            f"""
            Merge the following section-wise answers into ONE coherent response.
            Keep it concise but ensure all points are covered.  Preserve citations.
    
            Section answers:
            {pieces}
    
            ---
            Produce the merged answer below:
            """
        )
        merged = _llm.invoke(prompt).content.strip()
        state["master_answer"] = merged
        return state
    
    ```
    
    ---
    
    ## 3 ▪ **Sequential** sub-graph loop (aggregator swap only)
    
    If you prefer to stay sequential, simply:
    
    1. **Import** the new node in `agent/graph.py`
        
        ```python
        from agent.nodes import master_synth
        
        ```
        
    2. **Replace** the line that reused `synthesizer`:
        
        ```python
        g.add_node("master_synth", master_synth.master_synth)
        
        ```
        
    
    No other edits needed; the loop we built earlier now ends at `master_synth`, which
    
    reads `state["sub_summaries"]` and writes `state["master_answer"]`. The value is already available at `END`:
    
    ```python
    print(result["master_answer"])
    
    ```
    
    ---
    
    ## 4 ▪ **Parallel** sub-task execution (optional)
    
    If you want each sub-task to run **concurrently** (and you’re OK stepping partly outside LangGraph for the fan-out/fan-in), do the following:
    
    ### 4.1 Compile a *single-topic* DAG once
    
    ```python
    # agent/topic_graph.py
    from agent.graph import build_single_topic_graph   # function shown below
    topic_graph = build_single_topic_graph().compile()
    
    ```
    
    ### 4.2 Helper to run many sub-tasks in parallel
    
    ```python
    # agent/nodes/parallel_runner.py
    import asyncio
    from agent.topic_graph import topic_graph
    from agent.state import AgentState
    
    async def _run_topic(task: dict, user_query: str) -> str:
        """Invoke graph for ONE topic and return its synthesized answer text."""
        init_state = {
            "query_raw": f"{user_query} (Focus: {task['topic']})",
            "metadata": task["metadata"],
            "route": task["suggested_route"],
            "fallback": [],
        }
        res = await topic_graph.ainvoke(init_state)
        return res["final_answer"]
    
    async def async_subtasks(state: AgentState) -> AgentState:
        subtasks = state["sub_tasks"]
        user_q = state["query_raw"]
        coro = [_run_topic(t, user_q) for t in subtasks]
        state["sub_summaries"] = await asyncio.gather(*coro)
        return state
    
    ```
    
    ### 4.3 Wire the parallel runner into **main** graph
    
    ```python
    from agent.nodes.parallel_runner import async_subtasks
    g.add_node("parallel_runner", async_subtasks)
    
    # Replace the **entire** sequential loop section:
    g.add_conditional_edges(
        "planner",
        {
            "multi": "parallel_runner",   # fan-out → await gather
            "cypher": "cypher",
            "hybrid": "hybrid",
            "rag": "rag",
        },
        key="route",
    )
    
    g.add_edge("parallel_runner", "master_synth")
    
    ```
    
    **Important:**
    
    *Because `async_subtasks` is an `async def`, build the graph with*
    
    ```python
    compiled = g.compile(asyncio_graph=True)
    
    ```
    
    and invoke with:
    
    ```python
    import asyncio
    result = asyncio.run(compiled.ainvoke({"query_raw": question}))
    print(result["master_answer"])
    
    ```
    
    ### 4.4 Utility to build a stand-alone single-topic graph
    
    ```python
    # agent/graph.py
    def build_single_topic_graph() -> StateGraph[AgentState]:
        """A DAG without multi-topic logic; used by parallel runner."""
        g = StateGraph(AgentState)
        g.add_node("cypher", cypher.cypher)
        g.add_node("hybrid", hybrid.hybrid)
        g.add_node("rag", rag.rag)
        g.add_node("validator", validator.validator)
        g.add_node("synthesizer", synthesizer.synthesizer)
    
        g.set_entry_point("cypher")  # planner already decided best route
        for n in ["cypher", "hybrid", "rag"]:
            g.add_edge(n, "validator")
        g.add_conditional_edges("validator",
            {
                "pass": "synthesizer",
                "cypher": "cypher",
                "hybrid": "hybrid",
                "rag": "rag",
                "fail": END,
            },
            route_decider,
        )
        g.add_edge("synthesizer", END)
        return g
    
    ```
    
    ---
    
    ## 5 ▪ What Changed?
    
    | Feature | Sequential Loop | Parallel Runner |
    | --- | --- | --- |
    | Latency per N sub-tasks | **N ×** single-topic time | ~ **max**(task time) |
    | Code footprint | Very small | +1 helper module |
    | Memory / token usage | Same | Slightly higher (concurrency) |
    | Debug trace (LangSmith) | Linear DAG | One trace per topic |
    
    ---
    
    ### ✅ You now have:
    
    - A **master_synth** node that truly merges sub-answers.
    - Option (toggle) to process each topic **concurrently** for speed.
    
    Add or adjust prompts, thresholds, or concurrency limits as you tune the system.
    
    Let me know if you need profiling tips or deployment tweaks!
    
- adjusted prompt
    
    Below are **prompt upgrades** for every LLM-powered node in the agent.
    
    Each snippet includes (a) the **new template**, (b) the **rationale** for the changes, and (c) the **code patch** you can paste directly into the corresponding module.
    
    ---
    
    ## 1. Planner – `nodes/planner.py`
    
    ### New template `PLANNER_SYS`
    
    ```python
    PLANNER_SYS = """\
    You are PlanLLM, an expert routing assistant for an SEC-filing Q&A agent.
    Your job is ONLY to produce a valid JSON object with keys:
      route          : "cypher", "hybrid", "rag", or "multi"
      fallback       : ordered list of backup routes
      metadata       : {company, year, quarter, doc_type}
      sub_tasks      : optional [{topic, metadata, suggested_route}]
    
    **Rules**
    1. Prefer `"cypher"` when ALL critical metadata (company, year, quarter OR doc_type)
       is present AND the user asks for a specific fact, figure, or extract.
    2. Use `"hybrid"` when metadata filters exist BUT the user requests explanation,
       summary, comparison, or multi-section reasoning.
    3. Use `"rag"` when metadata is incomplete or the query is open-ended.
    4. Use `"multi"` when ≥2 distinct risk categories, time periods, or companies appear.
    5. Provide at most TWO fallback routes.
    6. Confidence is implicit in your routing order; do not add extra keys.
    
    Return ONLY valid JSON. Do NOT wrap in markdown.
    """
    
    ```
    
    ```python
    # Replace _classify_query + heuristics with ONE call:
    def planner(state: AgentState) -> AgentState:
        q = state["query_raw"]
        meta_raw = _extract_metadata(q)  # keep the regex helper if you like
    
        # Ask the LLM to decide the routing JSON
        prompt = (
            PLANNER_SYS
            + "\n\nUser query:\n"
            + q
            + "\n\nExtracted metadata (may be partial):\n"
            + json.dumps(meta_raw)
        )
        result = _llm.invoke(prompt).content.strip()
        plan = json.loads(result)
    
        state.update(plan)  # merges route, fallback, metadata, sub_tasks
        return state
    
    ```
    
    **Rationale**
    
    - Explicit JSON spec reduces hallucination.
    - Clear decision rules align with SEC domain.
    - Eliminates the brittle regex‐based `qtype` classifier.
    
    ---
    
    ## 2. Retrieval-Aware Self-Reflection – append to `nodes/validator.py`
    
    ```python
    _REFLECT_PROMPT = """\
    Rate the following retrieved passages for their usefulness to answer the user’s
    question on a scale 0–10 (0 = irrelevant, 10 = perfectly sufficient).
    Return ONLY the integer.
    
    User question:
    {question}
    
    Passages:
    {joined}
    """
    
    from langchain_openai import ChatOpenAI
    _llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    def _llm_score(question: str, passages: list[str]) -> int:
        joined = "\n---\n".join(passages[:5])
        p = _REFLECT_PROMPT.format(question=question, joined=joined)
        return int(_llm.invoke(p).content.strip())
    
    ```
    
    Then, inside `validator()`:
    
    ```python
    hits = state.get("retrievals", [])
    score = _llm_score(state["query_raw"], [h["text"] for h in hits])
    
    if score < 6:
        state["valid"] = False
    else:
        state["valid"] = True
    
    ```
    
    **Rationale**
    
    - Uses the LLM itself to judge retrieval sufficiency; more robust than static hit-count.
    
    ---
    
    ## 3. Synthesizer – `nodes/synthesizer.py`
    
    ### New prompt `SYNTH_PROMPT`
    
    ```python
    SYNTH_PROMPT = """\
    You are FinAnswer, a cautious financial analyst AI.
    Answer the user’s question ONLY from the provided passages.
    If the answer is not fully supported, say “I don’t have enough information.”
    Use *concise professional language* (≤ 4 sentences).
    Add bracketed numeric citations like [1], [3] pointing to the passages.
    
    User question:
    {question}
    
    Passages:
    {context}
    """
    
    ```
    
    ```python
    def synthesizer(state: AgentState) -> AgentState:
        ...
        prompt = SYNTH_PROMPT.format(
            question=state["query_raw"], context=context_chunks
        )
        answer = _llm.invoke(prompt).content.strip()
        ...
    
    ```
    
    **Rationale**
    
    - Enforces brevity, cites sources, and guards against unsupported assertions.
    
    ---
    
    ## 4. Master Aggregator – `nodes/master_synth.py`
    
    ### New prompt `AGG_PROMPT`
    
    ```python
    AGG_PROMPT = """\
    Combine the section-wise answers into ONE coherent response.
    Remove duplicates, keep logical order: Market Risk → Credit Risk → Operational Risk.
    Preserve citations exactly as they are.
    
    Section answers:
    {pieces}
    
    Write the merged answer (max 8 sentences):
    """
    
    ```
    
    ```python
    def master_synth(state: AgentState) -> AgentState:
        ...
        prompt = AGG_PROMPT.format(pieces=pieces)
        merged = _llm.invoke(prompt).content.strip()
        state["master_answer"] = merged
        return state
    
    ```
    
    **Rationale**
    
    - Gives explicit ordering and length cap; keeps citations intact.
    
    ---
    
    ## 5. Sub-Task Spawn Prompt (optional enhancement)
    
    If you let the LLM *also* split topics instead of regex:
    
    ```python
    SPLIT_PROMPT = """\
    Identify distinct sub-questions in the user query *if any*.
    Return JSON list of {topic, suggested_route}.
    
    Routes: "cypher", "hybrid", "rag".
    Example topics: Market Risk, Credit Risk, Liquidity, ESG, Outlook.
    
    User query:
    {query}
    """
    
    plan_topics = json.loads(_llm.invoke(SPLIT_PROMPT.format(query=q)).content)
    
    ```
    
    Then populate `state["sub_tasks"]` accordingly.
    
    ---
    
    ## How to integrate
    
    1. Replace the corresponding prompt strings and small code blocks.
    2. Remove any old regex-based `qtype` logic you no longer need.
    3. Rerun unit tests; update expected `route` values if the smarter planner changes decisions.
    
    ---
    
    ### 🚀 Result
    
    Your agent now:
    
    - Routes with an explicit JSON contract.
    - Uses LLM self-reflection to judge retrieval sufficiency.
    - Produces concise, well-cited answers.
    - Merges multi-topic outputs in a controlled order.
    
    Need more fine-tuning (temperature, max tokens) or domain-specific examples?
    
    Just let me know!